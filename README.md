本研究探究了Apache Hadoop MapReduce框架中Reduce任务的启动时机对作业性能的影响。通过设计和执行一系列对比实验，即设计了同种任务（WordCount）在不同数据规模（100MB、500MB、1GB）下，调整Reduce任务启动的比例参数（SlowStart: 0.2、0.5、0.8、1.0），系统地分析了作业执行时间、资源利用率等关键性能指标。实验结果表明，**Reduce任务启动时机的选择对MapReduce作业性能有显著影响**，不同的应用场景需要采用不同的启动策略以获得最优性能。

---

## 1. 引言

### 1.1 研究背景

Apache Hadoop是当前最流行的分布式计算框架之一，MapReduce是其核心的编程模型。在MapReduce作业中：

+ **Map阶段**处理输入数据，产生中间结果
+ **Shuffle阶段**负责数据的传输、排序和合并
+ **Reduce阶段**对中间结果进行聚合

传统的MapReduce实现中，Reduce任务通常等到**所有Map任务完成后**才开始执行。然而，现代的Hadoop版本（YARN）支持**提前启动Reduce任务**，使其在Map任务执行过程中就开始进行Shuffle和Sort操作。

### 1.2 研究意义

**问题所在：**

+ 当前没有明确的指导原则来确定在何种场景下应该采用哪种Reduce启动策略
+ 过早启动Reduce任务可能导致资源竞争和Map任务性能下降
+ 延迟启动Reduce任务可能导致集群资源利用不充分

**研究价值：**

+ 为MapReduce应用的性能优化提供数据支撑
+ 帮助用户根据应用特征选择合适的启动策略
+ 揭示Reduce启动时机与资源利用之间的内在关系

### 1.3 研究目的

**本研究目的为：** 探究MapReduce中Reduce任务的启动时机及其对作业性能的影响。

---

## 2. 研究内容

本研究主要围绕MapReduce的Reduce任务启动策略展开，在**研究目的**中已明确。具体研究内容包括：

#### 2.1 Reduce任务启动时机分析

Reduce任务的启动时机由参数 `mapreduce.job.reduce.slowstart.completedmaps` 控制。我们将分析此参数在不同取值下，MapReduce作业执行流程的变化以及可能带来的影响。

#### 2.2 性能指标定义

为量化评估不同启动策略的效果，本研究定义并测量以下关键性能指标：

+ **作业完成时间**： 从整个 MapReduce 任务开始，到最后一个 Reduce 任务完成为止的总耗时。  
+ **Map/Reduce平均完成时间**： 
  - 平均 Map 任务完成时间：所有 Map 任务的执行耗时的平均值。
  - 平均 Reduce 任务完成时间：所有 Reduce 任务的执行耗时的平均值。

可用于分析任务的负载是否均衡、是否存在性能瓶颈，也能反映调度策略是否合理。不同启动策略可能影响 		Reduce 阶段的整体耗时。  

+ **Shuffle+Sort耗时**： 指从 Map 输出数据开始传输给 Reduce 任务（Shuffle），并对数据进行排序（Sort）的总耗时。  
+ **CPU利用率**： MapReduce 作业运行期间，系统 CPU 的使用率  
+ **内存利用率**：作业执行过程中内存的使用百分比。

#### 2.3 实验设计

我们将采用多因素对比实验设计，涉及三个主要维度：工作负载类型（WordCount）、数据规模（100MB、500MB、1GB）和Reduce启动比例（SlowStart: 0.2、0.5、0.8、1.0）。通过系统地组合这些维度，单个实验运行多次取性能均值，在多实验下继续对比完成性能分析。

#### 2.4 实验结果与分析

对收集到的MapReduce日志、CPU性能日志进行数据提取、统计分析，通过图表和表格直观呈现结果。分析Reduce启动时机对MapReduce任务性能、CPU利用率、内存占用率的影响，并对不同工作负载和数据规模下的性能差异进行深入探讨。

#### 2.5 结论与实践建议

根据实验结果，总结不同Reduce启动策略的优劣，并针对不同应用场景（如延迟、吞吐、资源等）提出具体的实践优化建议和配置指南。

---

## 3. 实验设置

### 3.1 实验环境

#### 3.1.1 硬件配置

本实验在模拟的分布式集群环境下进行，共 **4个节点**：1个Master节点和3个Worker节点。

| 配置项       | Master节点                 | Worker节点 (3个)           |
| ------------ | -------------------------- | -------------------------- |
| **CPU核数**  | 2核                        | 2核                        |
| **内存大小** | 10GB DDR4                  | 10GB DDR4                  |
| **网络带宽** | 300Mbps                    | 300Mbps                    |
| **存储类型** | VMDK 虚拟磁盘（SCSI）  40G | VMDK 虚拟磁盘（SCSI）  40G |


#### 3.1.2 软件配置

集群的操作系统和核心软件栈配置如下：

| 软件            | 版本                              | 配置说明                           |
| --------------- | --------------------------------- | ---------------------------------- |
| **操作系统**    | Ubuntu20.04、Ubuntu22.04、CentOS7 | 64位，虚拟机                       |
| **JDK 版本**    | OpenJDK 1.8.0                     | 所有节点统一配置，各版本号略有不同 |
| **Hadoop 版本** | Apache Hadoop 3.3.4               | YARN (MRv2) 模式                   |
| **Python 版本** | 3.13.1                            | 用于数据集生成和数据分析脚本       |


### 3.2 实验负载

#### 3.2.1 数据集

本实验使用wiki上下载的数据集enwiki-latest-pages-articles.xml.bz2的切片。

+ **内容特征**：数据集是维基百科英文版 (English Wikipedia) 的最新完整数据转储 (dump)，具体包含的是文章页面，是大数据集。
+ **数据集规模**：100MB+500MB+1GB
+ **存储**：所有数据集已上传至HDFS。

#### 3.2.2 工作负载

本实验采用了典型的MapReduce工作负载：

+ **WordCount**：
  - **特点**：Map阶段进行词频计数，Reduce阶段进行聚合。Map和Reduce阶段的计算量相对平衡，Shuffle的数据量较大。
  - **代表性**：是MapReduce的“Hello World”程序，常用于评估通用批处理性能。

### 3.3 实验步骤

本实验的执行流程严格遵循预设的步骤，确保实验的可重复性和数据准确性。

**步骤1：集群环境初始化与验证**

1. **验证网络环境**：Master节点通过SSH连接到其余Worker节点。

![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321433822-a88ef07f-a165-4ae5-bbe6-f63a2ed5f240.png)

2. **启动Hadoop集群**：执行`start-dfs.sh`和`start-yarn.sh`命令启动HDFS和YARN服务。

![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321273507-8b45f0f5-87ef-4d41-b94a-23f82554c12c.png)

3. 验证集群状态：
   - 通过`jps`命令检查所有节点上的JVM进程，确保`NameNode`、`ResourceManager`、`DataNode`、`NodeManager`等核心进程均正常运行。
   - **【Master节点jps进程信息】**
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321323601-3e379362-6792-4c38-af76-58a46e78232d.png)
   - **【各Worker节点JPS进程信息】**
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321464983-f3abc1a1-c004-4ee5-974c-7333eb43d8ff.png)
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321798379-5798f2b7-8de1-487b-8d65-3f7766869c3b.png)
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321825635-c0bee17b-716c-4987-8608-7bf0f334ccc1.png)
   - **【hdfs dfsadmin -report指令****<font style="color:rgba(0, 0, 0, 0.95) !important;">查看集群的整体状态和详细资源信息</font>****】**
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321694368-28d617c7-dd10-468f-bc92-e33403eca755.png)
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321709742-0db09de9-fbeb-477b-8020-547c591ba2f8.png)
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321959925-e9a22861-8420-49a4-be83-47a2d68fcb4f.png)
   - ![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764321975800-95fba794-359b-4854-8920-b0fde574a72c.png)
4. **HDFS空间检查**：执行`hdfs dfs -df -h`确认HDFS有足够的存储空间。

![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764322037520-126b1f23-37d9-4849-ae45-0651bec30590.png)

**步骤2：数据集准备**

1. **原始数据集准备**：在wiki上下载的数据集enwiki-latest-pages-articles.xml.bz2的切片，切片大小包括100MB、500MB、1GB的文本文件。

![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764322184759-b28c1a2a-93f4-4f7f-860e-201f830d56c1.png)

2. **上传至HDFS**：将生成的原始数据集从本地文件系统上传到HDFS的`/user/hadoop/input`目录下，例如：`hdfs dfs -put /path/to/local/1GB_data.txt /user/hadoop/input/`。<font style="background-color:#FBDE28;"></font>![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764322073506-03a12924-4266-4aeb-a0b8-58b82f710c0d.png)
3. **验证HDFS文件**：执行`hdfs dfs -ls /user/hadoop/input`确认文件已成功上传。<font style="background-color:#FBDE28;"></font>

![](https://cdn.nlark.com/yuque/0/2025/png/63105379/1764322245312-a361007a-4666-4ffe-86ea-b97dfa0327bc.png)

**步骤4：实验代码构建**

**实验代码如下：**

[https://github.com/Littleleii/MapReduce511/tree/main/wheel](https://github.com/Littleleii/MapReduce511/tree/main/wheel)

 本实验为了系统地评估 Hadoop MapReduce 在不同 `mapreduce.job.reduce.slowstart.completedmaps` 参数下的性能表现，设计并实现了三个自动化脚本：monitor_real.sh、run_mr_real.sh 和 run_batch.sh。这三个脚本协同工作，实现了集群性能监控、MapReduce 任务自动执行、多组 slowstart 参数批量测试、日志自动归档等完整流程。  

**（1）monitor_real.sh：集群实时性能监控脚本**

`monitor_real.sh` 负责在 MapReduce 作业运行期间，**实时监控每个 Worker 节点的 CPU 利用率和内存占用情况**，并将数据记录到指定日志文件。实验需要在作业运行时记录集群资源消耗，Yarn 自身监控信息不足以满足精度要求、Hadoop Web UI 不便于保存、分析。脚本每隔 1 秒采样一次，包括：CPU 利用率、内存占用率、各节点的监控分隔记录、作业总运行时间

  	节点列表在脚本开头定义：

```plain
NODES=("worker1-zzh" "worker2-zrt" "worker3-haz")
```

脚本通过 SSH 登录 worker 节点读取：

+ `/proc/stat` —— 用于计算 CPU 使用率（两次采样差分法）
+ `/proc/meminfo` —— 用于计算内存使用百分比

该脚本由 `run_mr_real.sh` 自动启动，当 Yarn 检测到 MapReduce 作业结束后自动停止，监控数据写入对应时间戳目录中的 `monitor.log`。

---

**（2）run_mr_real.sh：单次 MapReduce 运行 + 性能监控脚本**

`run_mr_real.sh` 负责执行单次 MapReduce wordcount 作业，并在此过程中：自动对 HDFS 同名输出目录进行删除（避免报错）、自动启动监控脚本 monitor_real.sh、记录 MapReduce 作业的日志输出、将所有日志保存到对应的时间戳文件夹中。

本脚本接收 3 个参数：

```plain
./run_mr_real.sh <input_path> <output_path> <slowstart>
```

本脚本使用外部传入的环境变量 `RUN_LOG_DIR` 创建日志目录，并启动监控脚本：

```plain
monitor_real.sh → monitor.log
```

同时删除HDFS的旧输出：

```plain
hdfs dfs -rm -r -f "$OUTPUT"
```

运行 WordCount，并指定 slowstart 参数：

```plain
-D mapreduce.job.reduce.slowstart.completedmaps=$SLOWSTART
```

MapReduce 执行结果写入：

```plain
job_output.log
```

每次运行会生成一个独立目录，包含：

+ `monitor.log` —— 节点 CPU/MEM 时序数据
+ `job_output.log` —— MapReduce 作业日志（包括提交、运行、结束信息）

---

**（3） run_batch.sh：多 slowstart 参数批处理脚本**

`run_batch.sh` 控制整个实验流程，是本次 MapReduce 性能测试的总控脚本。它实现对不同 slowstart（0.2 / 0.5 / 0.8 / 1.0）自动执行 MapReduce，每种 slowstart 运行 3 次自动创建结构化日志目录，将每一次运行的结果保存到独立时间戳文件夹中

**s**lowstart 参数列表：

```plain
SLOWSTART_VALUES=(0.2 0.5 0.8 1.0)
```

每个 slowstart 运行次数

```plain
RUNS_PER_SS=3
```

**步骤5：数据处理文件**

**（1）common_utils.py：用于数据分析的工具库，解析日志文件，处理多轮实验等，供分析函数调用。**

**（2）analyze_all_metrices.py：用于生成各指标图表。**

**（3）analyze_cpu_slowstart.py：用于生成cpu利用率曲线。**

**（4）analyze_mem_slowstart.py：用于生成内存利用率曲线。**

---

## 4. 实验结果与分析

### 4.1 基于 Enwiki 100MB 数据集的MapReduce性能实验  

#### 4.1.1 任务整体性能分析  

本实验在 100MB 数据集上测试了不同 slowstart（SS）参数对 MapReduce 任务性能的影响，实验结果表明，**slowstart = 0.8** 在多数关键指标上表现最优，是本数据量下的最佳配置。

###### <font style="color:rgb(51, 51, 51);">指标一：任务总耗时 (单位: 秒)</font>

 从下表可以看到，在 100MB 数据集下，不同 SlowStart 参数对任务总耗时影响明显。随着 SS 从 0.2 提升到 0.8，作业总时间从 140.23s 逐步下降到 129.39s，说明适当推迟 Reduce 启动，有助于减少 Reduce 端的空等时间，使整体 pipeline 更紧凑。其中 **SS=0.8 的耗时最短，为 129.39s，被选为当前数据规模下的最佳配置**。相比之下，SS=1.0（即等待所有 Map 完成后再启动 Reduce）导致 Map 与 Reduce 基本串行，任务总耗时升至 166.61s，明显劣于其他三种设置  。

| **Dataset** | SS=0.2 | SS=0.5 | SS=0.8     | SS=1.0 | Best SS |
| :---------- | :----- | :----- | :--------- | :----- | :------ |
| **100MB**   | 140.23 | 135.66 | **129.39** | 166.61 | 0.8     |


###### <font style="color:rgb(51, 51, 51);">指标二：Map 阶段耗时 (单位: 秒)</font>

 在 100MB 数据集下，不同 SlowStart 参数不仅影响任务总耗时，也会显著改变 **Map 阶段本身的完成时间**。当 SS 从 0.2 提升到 0.5 和 0.8 时，Map 阶段耗时由 138.02s 降低到 126.47s 和 126.31s，缩短了约 8%～9%，说明适度推迟 Reduce 启动，可以减少早期 Shuffle/Reduce 对 Map 任务的资源抢占，使 Map 侧的 CPU、网络与 I/O 更集中地服务于前期计算，从而加快 Map 完成。而 SS=1.0 时 Map 耗时反而升至 148.26s，甚至高于 SS=0.2，表明过晚启动 Reduce 会导致整体 pipeline 不够紧凑，Map 在后期缺乏与 Shuffle/Reduce 的并行协同，拖长了阶段边界。因此，本实验中 **SS=0.8（或 0.5）在 Map 阶段表现最佳**，既避免了过早 Reduce 带来的资源干扰，又避免了过晚启动造成的阶段串行化。  

| **Dataset** | SS=0.2 | SS=0.5     | SS=0.8     | SS=1.0 | Best SS      |
| :---------- | :----- | :--------- | :--------- | :----- | :----------- |
| **100MB**   | 138.02 | **126.47** | **126.31** | 148.26 | 0.8 (或 0.5) |


###### 指标三：Reduce 阶段Copy/Shuffle耗时 (单位: 秒)

 在 100MB 数据集下，**Reduce 阶段 Copy/Shuffle 的“阶段耗时”随 SlowStart 增大而单调下降**：SS=0.2 时需要 83.97s，SS=0.5 时降到 64.14s，SS=0.8 时进一步缩短到 35.34s，而 SS=1.0 仅为 7.46s，是本指标下的“最佳值”。这说明当 Reduce 启动得越早（SS 越小），Reducer 在 Copy/Shuffle 阶段中有越长时间处于“边拉数据、边等待 Map 输出”的状态，导致该阶段时间被严重拉长，隐藏了大量空等开销；相反，SS=1.0 下只有在所有 Map 完成后才启动 Reduce，几乎不存在等待，Copy/Shuffle 在较短时间内集中完成，因此该阶段耗时最短。不过需要注意的是，**Copy/Shuffle 阶段变短并不等价于整体作业最优**：前面任务总耗时的结果已经表明，SS=1.0 由于完全失去 Map–Reduce 重叠，整体完成时间反而最长，因此在设计策略时需要在“Shuffle 等待时间”与“阶段重叠度”之间做权衡。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS |
| :-------- | :----- | :----- | :----- | :------- | :------ |
| **100MB** | 83.97  | 64.14  | 35.34  | **7.46** | 1.0     |


###### 指标四：Reduce 阶段Sort/Merge耗时 (单位: 秒)

 在 100MB 数据集下，**Reduce 阶段 Sort/Merge 的耗时对 SlowStart 参数的敏感度很高但并不单调**：SS=0.2 时耗时仅 0.58s，为四种设置中最短；SS=0.8 略高为 0.66s；而 SS=0.5 和 SS=1.0 却分别升到 3.34s 和 3.46s，是前两者的 5～6 倍，因此本指标下的最佳配置是 **SS=0.2**。这说明当 Reduce 启动较早时，Shuffle 数据的到达更为分散，Reducer 可以在拉取数据的过程中分批进行归并，最终的 Sort/Merge 阶段压力较小、耗时较短；而在 SS=0.5/1.0 下，大量中间结果在较短时间内“挤”到 Reduce 端，导致溢写段数增加、归并层数加深，使最终 Sort/Merge 阶段被拉长。  

| Dataset   | SS=0.2   | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :------- | :----- | :----- | :----- | :------ |
| **100MB** | **0.58** | 3.34   | 0.66   | 3.46   | 0.2     |


###### 指标五：Reduce 阶段Reduce计算耗时 (单位: 秒)

 在 100MB 数据集下，**Reduce 阶段真正用于 Reduce 计算（即用户自定义 reduce() 逻辑执行）的耗时整体非常小，但对 SlowStart 的变化呈现明显的非单调波动**。其中 SS=0.2 时 Reduce 计算耗时仅 0.82s，为四种设置中最短；SS=0.8 略高为 1.65s；而 SS=0.5 与 SS=1.0 分别升至 5.07s 和 4.90s，是前两者的数倍，因此本指标下的最佳配置为 **SS=0.2**。  

| Dataset   | SS=0.2   | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :------- | :----- | :----- | :----- | :------ |
| **100MB** | **0.82** | 5.07   | 1.65   | 4.90   | 0.2     |


###### 指标六：Reduce 阶段总耗时 (单位: 秒)

在 100MB 数据集下，**Reduce 阶段总耗时随着 SlowStart 参数增大而单调下降**：SS=0.2 时 Reduce 总耗时为 85.37s，SS=0.5 降到 72.54s，SS=0.8 进一步缩短为 37.66s，而 SS=1.0 仅剩 15.82s，是该指标下的最优配置。这与前面 Copy/Shuffle、Sort/Merge 的结果一致：Reduce 启动得越晚，等待 Map 输出的时间越少，更多工作集中在 Map 完成之后一次性完成，因此从“Reduce 自己的视角”看，阶段被压得又短又紧凑。

但需要强调的是，**Reduce 阶段越短并不代表作业整体越快**。

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :-------- | :----- | :----- | :----- | :-------- | :------ |
| **100MB** | 85.37  | 72.54  | 37.66  | **15.82** | 1.0     |


###### 指标七：网络I/O Shuffle传输总量 (单位: MB)

 从表中可以看到，四种 SlowStart 设置下 Shuffle 传输总量完全一致，均为 **43.69MB**，Best SS 因此标为 N/A。这个结果说明：**SlowStart 只改变 Reduce 何时开始拉取数据，但不会改变 Map 输出的中间结果总量**。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :----- | :----- | :----- | :----- | :------ |
| **100MB** | 43.69  | 43.69  | 43.69  | 43.69  | N/A     |


###### 指标八：网络I/O Shuffle传输速率 (单位: MB/s)

 该指标随 SlowStart 增大呈明显上升趋势：SS=0.2 为 **0.52MB/s**，SS=0.5 为 **0.68MB/s**，SS=0.8 提升到 **1.24MB/s**，而 SS=1.0 达到 **5.85MB/s**，是最高值，因此 Best SS 被标为 1.0。其含义是：**Reduce 启动越晚，Shuffle 数据传输越集中，网络带宽会在更短时间内被“突发式”占满，从而统计到更高的平均速率**；反之，SS 较小时 Shuffle 被分摊到更长时间窗口里，速率更低但更平滑。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS |
| :-------- | :----- | :----- | :----- | :------- | :------ |
| **100MB** | 0.52   | 0.68   | 1.24   | **5.85** | 1.0     |


###### 指标九：磁盘I/O Spill(溢写)记录数

 四种 SlowStart 下 Spill 记录数也完全一致，均为 **6,111,990**，Best SS 标为 N/A。这个结果说明：**Map 端溢写量主要由 Map 输出规模、缓冲区大小、排序/合并策略等决定，而 SlowStart 不会改变 Map 产生数据的量**；同时 Reduce 是否早启动，也不会反向影响 Map 的溢写行为。  

| Dataset   | SS=0.2    | SS=0.5    | SS=0.8    | SS=1.0    | Best SS |
| :-------- | :-------- | :-------- | :-------- | :-------- | :------ |
| **100MB** | 6,111,990 | 6,111,990 | 6,111,990 | 6,111,990 | N/A     |


###### 指标十：磁盘I/O 评价

**磁盘溢写（spill）是该作业在 100MB 规模下的固有现象，不随 Reduce 启动时机改变**。原因通常在于 Map 端输出的中间结果超过了内存缓冲区阈值，触发了 spill 到磁盘  

| Dataset   | SS=0.2       | SS=0.5       | SS=0.8       | SS=1.0       | Best SS |
| :-------- | :----------- | :----------- | :----------- | :----------- | :------ |
| **100MB** | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | N/A     |


###### 指标十一：性能分析 Shuffle重叠度 (%)

 从表中可以看到，SS=0.2、0.5、0.8 下 Shuffle 重叠度分别为 **99.04% / 98.76% / 97.81%**，均接近 100%，而 SS=1.0 为 **0%**。这说明在 SS<1.0 时 Reduce 会提前启动，Shuffle 阶段能够与 Map 阶段形成几乎完全的并行重叠；但当 SS=1.0（完全等 Map 结束后启动 Reduce）时，两阶段彻底串行，因此重叠度为零。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS              |
| :-------- | :----- | :----- | :----- | :------- | :------------------- |
| **100MB** | 99.04  | 98.76  | 97.81  | **0.00** | N/A (取决于优化目标) |


#### 4.1.2 CPU性能分析

###### 指标一：集群内存占用

 在 100MB 数据集下，集群平均内存利用率整体处于 25%–48% 的中低负载区间，作业前期随 Map 任务扩展逐步上升，并在 50–60s 进入峰值段（约 45%–48%），之后在 60–120s 形成稳定平台（约 35%–40%），最后在尾部阶段同步回落。四种 SlowStart 策略的内存曲线全程高度重合，仅有极小波动差异，表明 Reduce 启动时机对平均内存占用影响不显著。  

![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764525809289-c562e7fe-f2e8-4876-aacf-45eab0506f2f.png)

###### 指标二：集群CPU占用

在 100MB 数据集下，四种 SlowStart 的 CPU 利用率在作业前半段（0–80s）均迅速升至满载并维持高位，差异不明显；但在 80s 之后逐渐分化：SS=1.0 最早出现负载下滑并产生明显拖尾，反映 Reduce 启动过晚导致 Map–Reduce 串行与 pipeline 空洞；SS=0.2/0.5 虽保持较高重叠，但因 Reduce 启动偏早带来等待型空转，平均 CPU 利用率不及 SS=0.8。综合平均 CPU 利用率与尾部形态，**SS=0.8 在 100MB 场景下实现了最高且最连续的 CPU 负载，是最优 slowstart 设置**。

| **<font style="color:rgba(6, 8, 31, 0.88);">Dataset</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.2</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.5</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.8</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=1.0</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">Best SS</font>** |
| :----------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :----------------------------------------------------------- |
| <font style="color:rgba(6, 8, 31, 0.88);">100MB</font>       | <font style="color:rgba(6, 8, 31, 0.88);">74.70</font>      | <font style="color:rgba(6, 8, 31, 0.88);">72.04</font>      | **<font style="color:rgba(6, 8, 31, 0.88);">77.82</font>**  | <font style="color:rgba(6, 8, 31, 0.88);">71.28</font>      | <font style="color:rgba(6, 8, 31, 0.88);">0.8</font>         |


![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764525916158-8dc3ce9e-27fd-428e-813a-ffad1525ef32.png)

#### 4.1.3 Slowstart 参数对性能的影响总结  

<font style="color:rgba(6, 8, 31, 0.88);">SlowStart = 0.8 在“并行计算”和“资源争抢”之间找到了最佳平衡点。</font>

<font style="color:rgba(6, 8, 31, 0.88);">在Reduce 阶段在 Map 快结束时（完成 80%）提前启动 Shuffle，既没有像SlowStart = 0.2因为过早启动导致网络拥堵拖慢 Map，也没有如SlowStart = 1.0因为过晚启动导致 Map 做完后 Reduce 才开始冷启动，浪费了时间。</font>

综上，**用于本数据规模（100MB）时，slowstart=0.8 在计算资源利用、阶段重叠、任务时延方面达到了最优平衡，是整体性能最好的配置**。这一结果也说明，适度的 Reduce 提前启动有助于充分利用集群资源，而过早或过晚都会引发阶段等待、资源浪费或 pipeline 中断，从而降低性能。

### 4.2 基于 Enwiki 500MB 数据集的MapReduce性能实验  

#### <font style="color:rgb(51, 51, 51);">4.2.1 任务整体性能分析  </font>

###### <font style="color:rgb(51, 51, 51);">指标一：任务总耗时 (单位: 秒)</font>

 从表中可以看到，500MB 数据集下任务总耗时在不同 SlowStart 设置间差异显著。SS=0.2 时总耗时 **545.09s** 最短，为最佳配置；SS=0.5 略高为 **553.57s**；而 SS=0.8 与 SS=1.0 则明显变慢，分别达到 **638.51s** 和 **630.55s**。这说明在中等规模数据下，**更早启动 Reduce（SS=0.2）能够显著增强 Map 与 Shuffle/Reduce 的流水线并行，减少阶段空档，从而缩短整体作业时间**。相反，当 SS 提高到 0.8 或 1.0 时，Reduce 启动偏晚，虽然可能减少 Reduce 端等待，但 Map–Reduce 重叠窗口被压缩甚至部分串行化，导致总耗时明显上升。  

| Dataset   | SS=0.2     | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :--------- | :----- | :----- | :----- | :------ |
| **500MB** | **545.09** | 553.57 | 638.51 | 630.55 | 0.2     |


###### <font style="color:rgb(51, 51, 51);">指标二：Map 阶段耗时 (单位: 秒)</font>

 Map 阶段耗时的变化趋势与总耗时高度一致：SS=0.2 时 **535.44s** 最短，SS=0.5 为 **543.03s** 略慢；而 SS=0.8 和 SS=1.0 分别升至 **630.50s** 和 **615.66s**，显著拉长。因此 Best SS 同样指向 **0.2**。这表明在 500MB 场景下，**Reduce 过晚启动会反向拖慢 Map 阶段**：Map 输出在后期缺乏及时的 Shuffle 消化与流水线配合，可能引起缓冲压力、spill/merge 更集中甚至资源竞争结构失衡，导致 Map 收尾变慢；而 SS=0.2 通过更早形成 Shuffle 重叠，使 Map 输出被持续拉取与处理，阶段边界更平滑，从而加速 Map 完成。  

| Dataset   | SS=0.2     | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :--------- | :----- | :----- | :----- | :------ |
| **500MB** | **535.44** | 543.03 | 630.50 | 615.66 | 0.2     |


###### 指标三：Reduce 阶段Copy/Shuffle耗时 (单位: 秒)

 该阶段耗时随 SS 增大快速下降：SS=0.2 为 **423.62s**，SS=0.5 降到 **268.45s**，SS=0.8 进一步缩短为 **155.80s**，而 SS=1.0 仅 **3.17s**，因此本指标下 Best SS 为 **1.0**。其含义与 100MB 场景一致：**Reduce 启动越早，就越容易在 Copy/Shuffle 阶段长时间等待 Map 输出，阶段被“拉长”；启动越晚，Shuffle 越集中，等待越少，阶段越短**。但这里同样要注意：Copy/Shuffle 变短只是 Reduce 侧局部视角的“好看”，并不代表整体最优——由于 SS=1.0 牺牲了阶段重叠，最终反映为总耗时最差。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS |
| :-------- | :----- | :----- | :----- | :------- | :------ |
| **500MB** | 423.62 | 268.45 | 155.80 | **3.17** | 1.0     |


###### 指标四：Reduce 阶段Sort/Merge耗时 (单位: 秒)

 从表中可以看到，500MB 下 Sort/Merge 耗时在不同 SlowStart 设置间差异不大，但仍有清晰的最优点。SS=0.8 时耗时 **3.30s** 最短；SS=0.2 为 **3.41s** 接近；SS=0.5 上升到 **3.96s**；SS=1.0 最长为 **4.42s**，因此 Best SS 为 **0.8**。这说明在 500MB 场景里，**SS=0.8 的 Reduce 数据到达节奏更利于归并**：既避免 SS=0.2 早启动导致的多批次小归并，也避免 SS=1.0 晚启动带来的数据集中涌入、归并层级加深。虽然该阶段占总耗时比例很小，但它反映了 slowstart 对 Reduce 端 merge 压力分布的影响。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8   | SS=1.0 | Best SS |
| :-------- | :----- | :----- | :------- | :----- | :------ |
| **500MB** | 3.41   | 3.96   | **3.30** | 4.42   | 0.8     |


###### 指标五：Reduce 阶段Reduce计算耗时 (单位: 秒)

 该指标反映 Reduce 端真正执行 reduce() 逻辑的计算时间。结果显示 SS=0.8 最低，仅 **4.07s**；SS=1.0 为 **4.74s**；SS=0.2 与 SS=0.5 分别达到 **5.62s** 和 **5.87s**，因此 Best SS 为 **0.8**。这表明适度推迟 Reduce 启动（SS=0.8）可让 Reduce 计算阶段更加“紧凑且均衡”：数据到达既不至于过早分散导致调度/上下文开销变大，也不至于过晚集中形成局部热点，从而减少 Reduce 端计算时间。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8   | SS=1.0 | Best SS |
| :-------- | :----- | :----- | :------- | :----- | :------ |
| **500MB** | 5.62   | 5.87   | **4.07** | 4.74   | 0.8     |


###### 指标六：Reduce 阶段总耗时 (单位: 秒)

 Reduce 总耗时随 SS 增大呈显著下降：SS=0.2 为 **432.65s**，SS=0.5 降到 **278.28s**，SS=0.8 进一步压缩为 **163.18s**，而 SS=1.0 仅 **12.33s**，因此本指标下 Best SS 为 **1.0**。原因与 Copy/Shuffle 一致：**Reduce 启动越晚，等待 Map 输出的时间越少，阶段被压缩得越短**。但这属于 Reduce 局部视角的“最短”，并不能代表整体最优——前面总耗时结果已经说明 SS=1.0 会牺牲 Map–Reduce 重叠并导致任务整体变慢，因此 Reduce 总耗时指标需要与 Job Completion Time 综合权衡。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :-------- | :----- | :----- | :----- | :-------- | :------ |
| **500MB** | 432.65 | 278.28 | 163.18 | **12.33** | 1.0     |


###### 指标七：网络I/O Shuffle传输总量 (单位: MB)

 四种 SlowStart 设置下 Shuffle 传输总量完全一致，均为 **240.22MB**，Best SS 标为 N/A。该结果说明：**SlowStart 不会改变 Map 输出中间结果的总规模，因此 Shuffle 数据总量只由作业本身决定，与 Reduce 启动时机无关**。因此该指标在 slowstart 策略对比中没有区分能力，更多用于刻画作业负载强度（500MB 下 Shuffle 量远高于 100MB，与输入规模相符）。  

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :-------- | :----- | :----- | :----- | :----- | :------ |
| **500MB** | 240.22 | 240.22 | 240.22 | 240.22 | N/A     |


###### 指标八：网络I/O Shuffle传输速率 (单位: MB/s)

表中结果显示 Shuffle 平均传输速率随 SlowStart 增大呈急剧上升趋势：SS=0.2 为 **0.57MB/s**，SS=0.5 为 **0.89MB/s**，SS=0.8 提升到 **1.54MB/s**，而 SS=1.0 直接跃升到 **75.76MB/s**，远高于前三种设置，因此本指标下 Best SS 为 **1.0**。这说明 **Reduce 启动越晚，Shuffle 数据就越集中在 Map 结束后短时间内传输完成**，统计到更高的平均速率；而 SS 较小时 Shuffle 被摊到更长时间窗口里，速率更低但更平滑。需要注意的是，速率高不等同于整体更快：SS=1.0 的高带宽利用来自阶段串行与突发式传输，并不能抵消其总耗时变长的问题，因此该指标更多刻画“传输形态”。

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :-------- | :----- | :----- | :----- | :-------- | :------ |
| **500MB** | 0.57   | 0.89   | 1.54   | **75.76** | 1.0     |


###### 指标九：磁盘I/O Spill(溢写)记录数

 四种 SlowStart 设置下 Spill 记录数完全一致，均为 **32,739,712**，Best SS 为 N/A。该结果表明：**spill 行为主要由 Map 输出规模、缓冲区大小与排序溢写机制决定，而 Reduce 启动时机不会改变 Map 端中间结果产生的数量和节奏**，因此也不会改变 spill 的触发频率。在 slowstart 对比中，这一指标不具区分能力，只能说明 500MB 规模下 Map 端确实存在较大规模的溢写压力。  

| Dataset   | SS=0.2     | SS=0.5     | SS=0.8     | SS=1.0     | Best SS |
| :-------- | :--------- | :--------- | :--------- | :--------- | :------ |
| **500MB** | 32,739,712 | 32,739,712 | 32,739,712 | 32,739,712 | N/A     |


###### 指标十：磁盘I/O 评价

 表中四种 SS 的磁盘 I/O 评价均为“**存在磁盘溢写**”，Best SS 为 N/A，这与 spill 记录数一致。  

| Dataset   | SS=0.2       | SS=0.5       | SS=0.8       | SS=1.0       | Best SS |
| :-------- | :----------- | :----------- | :----------- | :----------- | :------ |
| **500MB** | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | N/A     |


###### 指标十一：性能分析 Shuffle重叠度 (%)

Shuffle 重叠度在 SS<1.0 下均接近 100%：SS=0.2 为 **99.85%**，SS=0.5 为 **99.73%**，SS=0.8 为 **99.60%**，而 SS=1.0 为 **0%**。这说明在 500MB 规模下，只要 Reduce 不是等到所有 Map 完成后才启动，Shuffle 都能与 Map 形成几乎完全的流水线重叠；而 SS=1.0 会导致两阶段彻底串行，完全失去重叠窗口。值得强调的是，三种“高重叠”设置之间差距极小，但你在总耗时与 Map 阶段耗时中观察到 SS=0.2 最优，这意味着 **重叠度只是必要条件而非充分条件**：真正决定整体性能的仍是 Reduce 启动过早带来的等待开销与过晚带来的并行损失之间的平衡，因此 Best SS 需要结合多指标综合优化，而不能仅由重叠度给出。

| Dataset   | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS              |
| :-------- | :----- | :----- | :----- | :------- | :------------------- |
| **500MB** | 99.85  | 99.73  | 99.60  | **0.00** | N/A (取决于优化目标) |


#### 4.2.2CPU性能分析

###### 指标一：集群平均内存占用 

 在 500MB 数据集下，集群平均内存利用率整体维持在约 25%–55% 区间。作业启动阶段（0–50s）内存快速上升并达到局部峰值，随后在 50–420s 形成长期稳定平台（约 40%–47%），四种 SlowStart 策略曲线几乎重合，仅有细微振荡差异；尾部阶段（约 420s 后）各策略内存同步回落并收敛。结果表明：在该规模下 Reduce 启动时机对平均内存占用影响不显著，内存指标区分策略优劣的能力有限，slowstart 的性能差异应主要结合作业总耗时、CPU 连续性及 Shuffle 等待等指标综合评估。  

![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764566907730-94d3dfdd-f802-47ca-8c53-577577a5a75d.png)

###### 指标二：集群CPU占用

 在 500MB 数据集下，作业在约 30s 后快速进入满载状态，CPU 利用率在主体阶段（≈30–400s）长期维持在接近 100%，四种 SlowStart 策略曲线几乎重合，表明该规模下流水线足够饱满，slowstart 对主干阶段 CPU 占用影响有限。策略差异主要集中在 400s 之后的尾部收敛阶段：SS=1.0 因 Reduce 启动过晚导致尾部拖尾与负载波动，平均 CPU 最低；SS=0.5 则在保证高重叠的同时减少等待空转，使尾部更紧凑、平均 CPU 利用率最高（91.98%），因此为该指标下的最佳 SlowStart 设置。  

| **<font style="color:rgba(6, 8, 31, 0.88);">Dataset</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.2</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.5</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.8</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=1.0</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">Best SS</font>** |
| :----------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :----------------------------------------------------------- |
| <font style="color:rgba(6, 8, 31, 0.88);">500MB</font>       | <font style="color:rgba(6, 8, 31, 0.88);">90.88</font>      | **91.98**                                                   | <font style="color:rgba(6, 8, 31, 0.88);">90.09</font>      | <font style="color:rgba(6, 8, 31, 0.88);">86.06</font>      | <font style="color:rgba(6, 8, 31, 0.88);">0.5</font>         |


![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764526046244-205ea616-0bd3-45d3-afbd-578ded74cff6.png)

#### **4.2.3 Slowstart 参数对性能的影响总结  **

综合来看：500MB 数据规模下，slowstart=0.2 更偏“总体性能最优”，适合追求最短任务总耗。slowstart=0.5 更适合集群资源利用最大化；slowstart=0.8 则在 Reduce 阶段表现最佳。比起 100MB 数据集的 “0.8 最优”，500MB 更适合提前启动 Reduce（SS=0.2 或 0.5），说明数据规模变大后 Shuffle 带宽占比提升，Reduce 提前启动反而更有利于缓存网络压力。

### 4.3 基于 Enwiki 1G 数据集的MapReduce性能实验  

#### 4.3.1 基于 Enwiki 1G 数据集的MapReduce性能实验

###### <font style="color:rgb(51, 51, 51);">指标一：任务总耗时 (单位: 秒)</font>

 从表中可以看到，1G 数据集下不同 SlowStart 的任务总耗时差异显著。SS=0.5 时总耗时 **847.96s** 最短，为最佳配置；SS=0.2 与 SS=0.8 的耗时接近，分别为 **856.64s** 与 **855.46s**，略慢于 SS=0.5；而 SS=1.0 的耗时高达 **1247.97s**，显著劣于其他策略。该结果说明在更大规模数据下，**适度提前 Reduce 启动（SS=0.5）能够在保持高 Shuffle 重叠的同时避免过早空等，从而实现最紧凑的流水线**；而 SS=1.0 由于 Reduce 过晚启动导致 Map–Reduce 串行化，阶段空档被放大，最终使整体作业时间大幅上升。  

| Dataset | SS=0.2 | SS=0.5     | SS=0.8 | SS=1.0  | Best SS |
| :------ | :----- | :--------- | :----- | :------ | :------ |
| **1GB** | 856.64 | **847.96** | 855.46 | 1247.97 | 0.5     |


######  <font style="color:rgb(51, 51, 51);">指标二：Map 阶段耗时 (单位: 秒)</font>

 Map 阶段耗时的趋势与总耗时一致：SS=0.5 时 Map 耗时 **837.76s** 最短，为最佳；SS=0.2 与 SS=0.8 分别为 **841.91s** 与 **840.93s**，差异不大；而 SS=1.0 直接升至 **1224.48s**，几乎比 SS=0.5 多出 45% 以上。该现象表明，**Reduce 启动时机不仅影响 Reduce 自身，还会反向影响 Map 收尾效率**：当 SS=1.0 过晚启动 Reduce，Map 输出在后期无法被及时 Shuffle 消化，容易带来 spill/merge 的集中压力或管线空洞，使 Map 阶段被拖长；而 SS=0.5 让 Shuffle 更早参与流水线，减少 Map 末端阻塞，从而缩短 Map 完成时间。  

| Dataset | SS=0.2 | SS=0.5     | SS=0.8 | SS=1.0  | Best SS |
| :------ | :----- | :--------- | :----- | :------ | :------ |
| **1GB** | 841.91 | **837.76** | 840.93 | 1224.48 | 0.5     |


###### 指标三：Reduce 阶段Copy/Shuffle耗时 (单位: 秒)

 Copy/Shuffle 阶段耗时随 SS 增大快速下降：SS=0.2 为 **668.91s**，SS=0.5 降到 **415.02s**，SS=0.8 进一步缩短到 **184.45s**，而 SS=1.0 仅 **12.40s**，因此该指标下 Best SS 为 **1.0**。这说明 **Reduce 启动越早，越容易在 Shuffle 期长时间等待 Map 输出，阶段被显著拉长；启动越晚，Shuffle 越集中、等待越少，阶段越短**。但需要注意，Copy/Shuffle 的“最短”并不等价于作业整体最优——SS=1.0 虽然让 Shuffle 极度集中、阶段耗时最小，却牺牲了与 Map 的重叠，导致总耗时最差。因此该指标应作为“Reduce 等待程度”的刻画，而不是最终策略选择的唯一依据。  

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :------ | :----- | :----- | :----- | :-------- | :------ |
| **1GB** | 668.91 | 415.02 | 184.45 | **12.40** | 1.0     |


###### 指标四：Reduce 阶段Sort/Merge耗时 (单位: 秒)

 表中显示，1G 数据集下 Sort/Merge 耗时在不同 SlowStart 设置间存在明显差异。SS=0.5 时耗时 **1.54s** 最短，为最佳配置；SS=1.0 次之为 **1.63s**；SS=0.8 为 **2.93s**；SS=0.2 则最高达到 **3.48s**。这说明在 1G 规模下，**SS=0.5 能让 Shuffle 数据到达节奏最有利于 Reduce 端归并**：既避免 SS=0.2 早启动导致的多批次小规模 merge（归并次数多、开销累积），也避免 SS=0.8 启动偏晚带来的中间结果相对集中、归并层级加深。由于该阶段耗时本身只有 1–3 秒量级，对总耗时贡献较小，但它能反映 Reduce 侧数据聚合形态的差异。  

| Dataset | SS=0.2 | SS=0.5   | SS=0.8 | SS=1.0 | Best SS |
| :------ | :----- | :------- | :----- | :----- | :------ |
| **1GB** | 3.48   | **1.54** | 2.93   | 1.63   | 0.5     |


###### 指标五：Reduce 阶段Reduce计算耗时 (单位: 秒)

 Reduce 计算耗时反映真正执行 reduce() 逻辑的 CPU 计算时间。结果显示 SS=1.0 时最低，仅 **7.27s**，因此 Best SS 为 **1.0**；SS=0.5 为 **7.93s** 接近；SS=0.8 和 SS=0.2 分别为 **10.65s** 与 **11.17s**，明显更高。这表明随着 SS 增大，Reduce 端计算负载更集中、等待更少，使统计到的计算阶段更短；而 SS=0.2/0.8 下 Reduce 数据到达更分散或存在更多等待/调度开销，导致计算持续时间被拉长。不过同样要强调：该阶段耗时虽然在几个到十几秒，但与 800–1200 秒级的总作业相比仍属次要指标，它更多体现 Reduce 工作分布的“形态”，不单独决定整体最优。  

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS |
| :------ | :----- | :----- | :----- | :------- | :------ |
| **1GB** | 11.17  | 7.93   | 10.65  | **7.27** | 1.0     |


###### 指标六：Reduce 阶段总耗时 (单位: 秒)

 Reduce 总耗时随 SlowStart 增大呈强烈单调下降：SS=0.2 为 **683.56s**，SS=0.5 降为 **424.49s**，SS=0.8 为 **198.03s**，而 SS=1.0 仅 **21.30s**，因此该指标下 Best SS 为 **1.0**。原因与 Copy/Shuffle 一致：**Reduce 启动越早，Shuffle 等待越长，阶段被显著拉长；启动越晚则几乎无等待，Reduce 在短窗口内集中完成**。但这仍是 Reduce 局部视角的“最短”，不能代表整体最优——SS=1.0 虽让 Reduce 自己很短，却牺牲 Map–Reduce 重叠，导致任务总耗时最差，因此该指标需与总耗时与 CPU 连续性结合解释。  

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :------ | :----- | :----- | :----- | :-------- | :------ |
| **1GB** | 683.56 | 424.49 | 198.03 | **21.30** | 1.0     |


###### 指标七：网络I/O Shuffle传输总量 (单位: MB)

 该结果说明：**Shuffle 数据总量只由作业本身的 Map 输出规模决定，与 Reduce 启动时机无关**；slowstart 只能改变传输发生的时间分布（集中/分散），不会改变数据“传多少”。  

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0 | Best SS |
| :------ | :----- | :----- | :----- | :----- | :------ |
| **1GB** | 442.15 | 442.15 | 442.15 | 442.15 | N/A     |


###### 指标八：网络I/O Shuffle传输速率 (单位: MB/s)

 表中可以看到 Shuffle 平均传输速率随着 SlowStart 增大明显上升：SS=0.2 为 **0.66MB/s**，SS=0.5 为 **1.07MB/s**，SS=0.8 升至 **2.40MB/s**，而 SS=1.0 达到 **35.66MB/s**，远高于前三者，因此本指标下 Best SS 为 **1.0**。这说明 **Reduce 启动越晚，Shuffle 传输越集中，网络带宽在更短时间内被突发式占满，从而形成更高的平均速率**；反之，SS 较小时 Shuffle 被分摊到更长的窗口里，速率更低但传输更平滑。需要强调的是，这种“速率最优”属于传输形态的局部指标，并不代表整体作业效率最优（总耗时 SS=1.0 反而最差），因此应结合重叠度和总耗时综合解读。  

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0    | Best SS |
| :------ | :----- | :----- | :----- | :-------- | :------ |
| **1GB** | 0.66   | 1.07   | 2.40   | **35.66** | 1.0     |


###### 指标九：磁盘I/O Spill(溢写)记录数

 该结果说明：**spill 主要由 Map 输出规模、缓冲区大小与溢写策略决定，Reduce 启动的早晚不会改变 Map 端生成中间结果的总量与节奏。**

| Dataset | SS=0.2     | SS=0.5     | SS=0.8     | SS=1.0     | Best SS |
| :------ | :--------- | :--------- | :--------- | :--------- | :------ |
| **1GB** | 61,021,742 | 61,021,742 | 61,021,742 | 61,021,742 | N/A     |


###### 指标十：磁盘I/O 评价

 这说明在 1G 数据规模下，**磁盘溢写是稳定且必然发生的过程**，并不因 Reduce slowstart 不同而改变。  

| Dataset | SS=0.2       | SS=0.5       | SS=0.8       | SS=1.0       | Best SS |
| :------ | :----------- | :----------- | :----------- | :----------- | :------ |
| **1GB** | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | 存在磁盘溢写 | N/A     |


###### 指标十一：性能分析 Shuffle重叠度 (%)

 Shuffle 重叠度在 SS<1.0 下几乎达到完全重叠：SS=0.2 为 **99.99%**，SS=0.5 为 **99.82%**，SS=0.8 为 **99.49%**，而 SS=1.0 为 **0%**。这说明在 1G 大规模作业中，只要 Reduce 不完全延后（SS<1.0），Shuffle 都能够与 Map 形成近乎 100% 的并行流水线；但 SS=1.0 会使 Shuffle 完全落在 Map 之后，从而重叠度归零。需要注意的是，虽然 SS=0.2 的重叠度最高，但总耗时最佳却是 SS=0.5，这再次表明 **高重叠度是必要条件而非充分条件**

| Dataset | SS=0.2 | SS=0.5 | SS=0.8 | SS=1.0   | Best SS              |
| :------ | :----- | :----- | :----- | :------- | :------------------- |
| **1GB** | 99.99  | 99.82  | 99.49  | **0.00** | N/A (取决于优化目标) |


#### 4.3.2 CPU性能分析

###### 指标一：集群平均内存占用

 在 1G 数据集下，集群平均内存利用率整体维持在 35%–45% 的中等水平，作业启动初期快速上升到平台区间，随后在 5s–50s 的主体阶段四种 SlowStart 策略曲线高度重合，仅出现更频繁的锯齿状波动，反映更大规模 Shuffle/merge 带来的周期性缓冲涨落。不同策略的差异主要集中在 50s 之后的尾部阶段，表现为内存下降的时刻与持续时长不同  

![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764525993577-140510cd-af9e-40a4-8fe2-689db740bda5.png)

###### 指标二：集群CPU占用

 在 1G 数据集下，CPU 利用率在作业启动后迅速升至接近 100%，并在约 2–50s 的主体阶段持续满载，四种 SlowStart 策略曲线几乎重合，说明在大规模负载下作业主干计算完全饱和，Reduce 启动时机对主体阶段 CPU 利用率影响很弱。策略差异主要体现在尾部收敛阶段：SS=0.2 与 SS=0.5 能更长时间维持高负载、尾部下降更晚，表明提前启动 Reduce 有助于延长 Shuffle 重叠并形成更紧凑的 pipeline；而 SS=0.8 与 SS=1.0 更早进入低负载收尾，反映 Reduce 启动偏晚导致 tail 变长与资源空洞增加。  

| **<font style="color:rgba(6, 8, 31, 0.88);">Dataset</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.2</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.5</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=0.8</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">SS=1.0</font>** | **<font style="color:rgba(6, 8, 31, 0.88);">Best SS</font>** |
| :----------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------- | :----------------------------------------------------------- |
| <font style="color:rgba(6, 8, 31, 0.88);">1G</font>          | **<font style="color:rgba(6, 8, 31, 0.88);">96.06</font>**  | <font style="color:rgba(6, 8, 31, 0.88);">94.71</font>      | <font style="color:rgba(6, 8, 31, 0.88);">92.64</font>      | <font style="color:rgba(6, 8, 31, 0.88);">95.37</font>      | <font style="color:rgba(6, 8, 31, 0.88);">0.2</font>         |


![](https://cdn.nlark.com/yuque/0/2025/png/35090272/1764526092401-fdd8da8a-056e-4bd0-987d-28083c32513d.png)

#### 4.3.3 Slowstart 参数对性能的影响总结  

在 1G 数据规模下，slowstart=0.5 在 Map 阶段耗时与任务总耗时方面表现最佳，同时保持较高的 Shuffle–Map 重叠度，是整体性能最优的配置。随着数据规模增大，适度延迟 Reduce 启动可以避免过度的网络争抢，使 Map 阶段更稳定，从而提升整个 MapReduce 流水线的效率。



---

## 5. 结论

第4节对比分析了不同slowstart参数在三种数据规模（100MB、500MB、1GB）下对 MapReduce 各阶段性能、Shuffle 行为、CPU以及内存利用率以及任务总耗时的影响。综合实验结果表明：**slowstart参数对 MapReduce性能有显著的影响，且最优值随数据规模变化而变化**。整体表现呈现出一个清晰规律：  

+ **小数据量（100MB）适合较小的 slowstart**，也即Reduce需要尽早启动，但同时在真实的实验环境下，Map阶段是否稳定、是否均匀、是否受到shuffle抢宽带也会影响作业运行的性能。
+ **中等数据量（500MB）适合中等的slowstart**，平衡 Shuffle–Map 的负载与网络压力。
+ **大数据量（1GB）适合偏晚的slowstart**，既保证Map阶段稳定，又避免Shuffle过早带来的网络拥塞。

实验发现随着数据规模从 100MB 到1GB 逐步增大，最优 SlowStart 发生了明确的变化：

| Dataset   | 最优 SS     | 主要原因                                                     |
| --------- | ----------- | ------------------------------------------------------------ |
| **100MB** | **0.8**     | Map 阶段短，资源空闲多，Reduce 早启动可提高流水化效率        |
| **500MB** | **0.2–0.5** | 数据量变大后，提前启动 Reduce 能分担部分网络传输，但过早会与 Map 抢占资源 |
| **1GB**   | **0.5**     | 数据量大，必须避免 Reduce 过早启动造成网络拥塞，使 Map 阶段变不稳定 |


**结论1：最优SlowStart参数具有数据规模相关性，不存在固定值。**

**对于100MB：提前启动有利**

+ Map 阶段很短，机器空闲资源多。
+ Reduce 早启动可以立刻接收 Shuffle 数据，使 pipeline 更紧凑。
+ 减少 Map→Shuffle→Reduce 间的等待与时延。

 因此SS=0.8效果最好。

**对于500MB：过早启动反而造成拥塞**

+ Reduce 若太早启动（SS=0.2），会与 Map 抢网络资源，但此时部分 Map 已完成，可适度提前帮助传输。
+ SS=0.8 太晚，不能利用前期带宽，导致等待增多。

中等数据量下，SS 过大或过小都会降低效率。

**对于1GB：需显著延迟启动**

+ 数据量大，网络拥塞风险高。
+ Reduce 启动过早会强迫 Map 推送大量数据导致 Map 执行时间变长（网络被抢占）。
+ SS≈0.5 能保证至少一半 Map 完成再启动 Reduce，保证 Map 阶段稳定性。

**结论2：Reduce 的最佳启动时间取决于 Map 阶段的压力与网络负载情况。**

实验显示：

+ **SS=1.0 的 Shuffle 重叠始终为 0（无重叠）**，此时 Reduce 必须等待所有 Map 完成才启动——虽然数据上 Shuffle 阶段=0，但实际是**最差性能**（因为等了太久）。
+ **SS=0.2、0.5、0.8 的重叠比例均可达到 100%**，但：
  - 重叠比例高不代表整体性能最好。
  - 例如在500MB下SS=0.5的重叠比例是 99.6%，但耗时却比 SS=0.2 更长。

**结论3：Shuffle–Map 重叠比例不能作为判断好性能的核心指标。真正关键的是Reduce 启动时机是否匹配 Map 阶段的资源压力。**

在三个数据规模下，**最佳 SS 对应的 CPU 利用率也最高**：

 - 100MB：SS=0.8（77.82%）
 - 500MB：SS=0.5（91.98%）
 - 1GB：SS=0.2（96.06%）

**结论4：最佳 SlowStart 实际上是在最大化 CPU 有效利用率，让 Map、Shuffle、Reduce 三段尽可能并行而非互相等待。**

## 6. 分工

| 姓名       | 学号        | 具体工作内容                                         | 贡献度 (%) |
| ---------- | ----------- | ---------------------------------------------------- | ---------- |
| **王茜蕾** | 51285903124 | 集群设计、环境搭建、运行脚本构建、数据分析、报告撰写 | 25%        |
| **周珠晗** | 51285903123 | 环境搭建、数据分析、报告撰写                         | 25%        |
| **张睿桐** | 51285903135 | 环境搭建、数据分析、报告撰写                         | 25%        |
| **何奡喆** | 51285903132 | 环境搭建、数据分析、图表生成代码构建、报告撰写       | 25%        |

